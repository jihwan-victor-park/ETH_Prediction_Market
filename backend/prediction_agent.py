"""
Prediction Agent for Crypto Market Analysis
Uses Groq's Llama 3.3 70B to provide market predictions and analysis
"""

from typing import List, Dict
from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
import os


class PredictionAgent:
    """AI Agent specialized in cryptocurrency market predictions"""

    def __init__(self):
        self.llm = ChatGroq(
            model="llama-3.3-70b-versatile",
            temperature=0.7,
            api_key=os.getenv("GROQ_API_KEY")
        )

        # ì—ì´ì „íŠ¸ì˜ í˜ë¥´ì†Œë‚˜ ì •ì˜
        self.system_prompt = SystemMessage(
            content="""You are a professional cryptocurrency market analyst and prediction expert.

Your role:
- Analyze cryptocurrency markets (Bitcoin, Ethereum, etc.)
- Provide data-driven predictions based on technical analysis
- Explain market trends in simple terms
- Give balanced perspectives (both bullish and bearish scenarios)
- Always mention that predictions are not financial advice

Your style:
- Simple and easy to understand with bullet poits
- Don't use emojis (ğŸ“ˆ ğŸ“‰ ğŸ’¡)
- Provide specific timeframes when making predictions
- Always acknowledge market uncertainty

Remember: Never guarantee profits. Markets are unpredictable."""
        )

    def chat(self, user_message: str, chat_history: List[Dict[str, str]] = None) -> str:
        """
        Process a chat message and return AI response

        Args:
            user_message: The user's question/message
            chat_history: Previous conversation history

        Returns:
            AI agent's response as a string
        """
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [self.system_prompt]

        # ì´ì „ ëŒ€í™” ë‚´ì—­ ì¶”ê°€ (ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
        if chat_history:
            for msg in chat_history:
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    messages.append(AIMessage(content=msg["content"]))

        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append(HumanMessage(content=user_message))

        # LLM í˜¸ì¶œ
        response = self.llm.invoke(messages)

        return response.content


def get_prediction(question: str, history: List[Dict[str, str]] = None) -> str:
    """
    Simple function interface for predictions

    Args:
        question: User's question about crypto predictions
        history: Optional chat history for context

    Returns:
        AI prediction response
    """
    agent = PredictionAgent()
    return agent.chat(question, history)


# CLI í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    agent = PredictionAgent()

    print("ğŸ¤– Crypto Prediction Agent")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "Bitcoinì´ ë‹¤ìŒ ì£¼ì— ì–´ë–»ê²Œ ë  ê²ƒ ê°™ì•„?",
        "Ethereumì´ $5000 ë„˜ì„ ìˆ˜ ìˆì„ê¹Œ?",
        "ì§€ê¸ˆ íˆ¬ìí•˜ê¸° ì¢‹ì€ ì‹œê¸°ì•¼?"
    ]

    history = []

    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ’¬ ì§ˆë¬¸ {i}: {question}")
        response = agent.chat(question, history)
        print(f"ğŸ¤– ë‹µë³€: {response}\n")

        # ëŒ€í™” ë‚´ì—­ì— ì¶”ê°€
        history.append({"role": "user", "content": question})
        history.append({"role": "assistant", "content": response})
